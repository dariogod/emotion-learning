{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class SentimentLabel(Enum):\n",
    "    NEGATIVE = 0\n",
    "    POSITIVE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class EmotionInfo(BaseModel):\n",
    "    arousal: float = Field(ge=0, le=1, description=\"Level of energy/activation in the emotion, from calm (0) to excited (1)\")\n",
    "    valence: float = Field(ge=0, le=1, description=\"Pleasantness of the emotion, from negative (0) to positive (1)\")\n",
    "    intensity: float = Field(ge=0, le=1, description=\"Overall strength of the emotional response, from weak (0) to strong (1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "emotion_info_schema = json.dumps(EmotionInfo.model_json_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "from anthropic import Anthropic\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def get_emotion_info(\n",
    "        input_text: str, \n",
    "        parse_error: str | None = None, \n",
    "        previous_output: str | None = None,\n",
    "        try_count: int = 0,\n",
    "        max_retries: int = 3\n",
    "    ):\n",
    "    client = Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "    \n",
    "    prompt = f\"\"\"Analyze the emotional content of this text and output a JSON object with the following schema:\n",
    "    {emotion_info_schema}\n",
    "    \n",
    "    Only output valid JSON, nothing else.\n",
    "    \n",
    "    Text to analyze: {input_text}\"\"\"\n",
    "\n",
    "    if parse_error:\n",
    "        prompt += f\"You already outputted the following JSON, but it was invalid:\\n{previous_output}\\nValidation errors: {parse_error}\\nPlease fix the errors and output a valid JSON.\"\n",
    "\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=1024,\n",
    "        messages=[{\n",
    "            \"role\": \"user\", \n",
    "            \"content\": prompt\n",
    "        }]\n",
    "    )\n",
    "    try:\n",
    "        response_json = json.loads(message.content[0].text)\n",
    "        return response_json\n",
    "    except json.JSONDecodeError as e:\n",
    "        return get_emotion_info(input_text, str(e), message.content[0].text, try_count + 1, max_retries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now at scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dariogod/coding/ugent/emotion-learning/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"nyu-mll/glue\", \"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"sentence\": \"hide new secretions from the parental units \",\n",
      "    \"label\": 0,\n",
      "    \"idx\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(ds[\"train\"][0], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_emotion_info_async(\n",
    "        input_text: str, \n",
    "        parse_error: str | None = None, \n",
    "        previous_output: str | None = None,\n",
    "        try_count: int = 0,\n",
    "        max_retries: int = 3\n",
    "    ):\n",
    "    client = anthropic.AsyncAnthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "    \n",
    "    prompt = f\"\"\"Analyze the emotional content of this text and output a JSON object with the following schema:\n",
    "    {emotion_info_schema}\n",
    "    \n",
    "    Only output valid JSON, nothing else.\n",
    "    \n",
    "    Text to analyze: {input_text}\"\"\"\n",
    "\n",
    "    if parse_error:\n",
    "        prompt += f\"You already outputted the following JSON, but it was invalid:\\n{previous_output}\\nValidation errors: {parse_error}\\nPlease fix the errors and output a valid JSON.\"\n",
    "\n",
    "    message = await client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=1024,\n",
    "        messages=[{\n",
    "            \"role\": \"user\", \n",
    "            \"content\": prompt\n",
    "        }]\n",
    "    )\n",
    "    try:\n",
    "        response_json = json.loads(message.content[0].text)\n",
    "        return response_json\n",
    "    except json.JSONDecodeError as e:\n",
    "        if try_count >= max_retries:\n",
    "            raise e\n",
    "        return await get_emotion_info_async(input_text, str(e), message.content[0].text, try_count + 1, max_retries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"augmented_data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def process_batch(batch_start, batch_size, ds):\n",
    "    sem = asyncio.Semaphore(2)  # Limit to 5 concurrent tasks\n",
    "    augmented_batch = []\n",
    "    \n",
    "    async def process_item(item):\n",
    "        async with sem:\n",
    "            emotion_info = await asyncio.gather(\n",
    "                get_emotion_info_async(item[\"sentence\"]),\n",
    "            )\n",
    "            return {\n",
    "                \"idx\": item[\"idx\"],\n",
    "                \"sentence\": item[\"sentence\"],\n",
    "                \"emotion_info\": emotion_info,\n",
    "                \"label\": item[\"label\"]\n",
    "            }\n",
    "\n",
    "    # Create tasks for all items in the batch\n",
    "    tasks = [\n",
    "        process_item(ds[\"train\"][i]) \n",
    "        for i in range(batch_start, batch_start + batch_size)\n",
    "    ]\n",
    "    \n",
    "    # Wait for all tasks to complete and collect results\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    augmented_batch.extend(results)\n",
    "    \n",
    "    return augmented_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "TOTAL_SIZE = 1000\n",
    "BATCH_START = 40\n",
    "\n",
    "for batch_start in range(BATCH_START, TOTAL_SIZE, BATCH_SIZE):\n",
    "    augmented_batch = await process_batch(batch_start, BATCH_SIZE, ds)\n",
    "    with open(f\"augmented_data/batch_{batch_start}-{batch_start+BATCH_SIZE}.json\", \"w\") as f:\n",
    "        json.dump(augmented_batch, f)\n",
    "    # Wait 60 seconds before processing next batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
